These are notes for making 3D point clouds from stereo images
summarized from browsing papers in the literature and reviews
and tutorials.
   Rectification is outlined briefly, then the making of disparity maps,
   followed by the visualization of the 3-D point clouds.
========================================================================

--------------
Rectification:
--------------
Once a set of point correspondances between images have been derived,
and the stereo projection transformation solved from those, the 
transformation parameters are used to create rectified left and right 
images where the epipolar lines are parallel to the horizontal axis.
This places both images in the same reference frame.

   There are 3 algorithms for rectification:
      -- planar (Fusiello et al. 2000;  Trucco and Verri 1998):
      -- cylindrical rectification (Oram 2001):
      -- polar rectification (Pollefeys et al. 1999):
   But there are also methods which use the epipolar geometry to rectify
   the images and these can be used if there are enough correspondence

   If the transformation matrix is still normalized, one can use the
   extrinsic parameters rotation and translation:

       "Theory and practice of projective rectification", 1991 by Hartley
       Hartley 1991 offers a method of rectifying pairs of images by making
       matched epipolar projections, that is projections in which the epipolar
       lines run parallel with the x-axis and the epipoles are mapped to infinity.
       The chosen projective transform is as close to rigid as possible 
       (that is, rotation and translation only).
           The transformation       | 1   0   0 |
                                G = | 0   1   0 |
                                    |-1/f 0   1 |
            transforms the epipole (f, 0, 1)^T  to the point at infinity (f, 0, 0)^T.
            A point (u, v, 1)^T is mapped by G to the point (u, v, 1 − u/f)^T. 
            If |u/f| < 1 then we may write (u, v, 1 − u/f) = (u(1 + u/f + ...), v(1 + u/f + ...),1)^T

            The Jacobian is
                ∂(u_transformed, v_transformed)     ( 1 + 2*u/f      0   )
                -------------------------------  =  (   v/f      1 + u/f )
                            ∂(u, v)

            One wants to avoid a non-quasi-affine projectivity.

            The transformation H' = G*R*T where T is a translation taking a point u0 to
            the origin and R is a rotation about the origin which puts the epipole
            point p at (f, 0, 1)^T on the x-axis.  G takes that point to infinity.
            The composite mapping is to first order a rigid transformation in the 
            neighbourhood of u0.

            Then the matching projective transformation H that minimizes the least-squares
            distance  summation_i of d(H*u_i, H'*u'_i)
            knowing H' and M, the matched points u'_i <--> u_i
            from the first image, vector u_i = (u_i, v_i, 1), so need to minimize
                summation_i of (a*u_i + b*v_i + c-u'_i^2)^2
                can find a, b, and c through linear least squares

                      then          | a   b   c |
                                A = | 0   1   0 |
                                    | 0   0   1 |

		      then H = (I + H'*p'*a^T)*H'*M

            Then resample the first image according to the projective 
            transformation H and the second image according to the 
            projective transformation H'.

       Other methods for rectifying images are in:
           "Rectifying transformations that minimize resampling effects", 2001 
                by Gluckman and Nayar
           "Multiple view image rectification", 2011 by Nozick

      Note that the rectified images now are aligned by y coordinate too, that is a matching
      point in one image has the same y coordinate in the other image, though the
      x will differ by disparity.
  
-------------------------
Making of Disparity Maps:
-------------------------

Given rectified stereo images, one can learn the relative distance of
objects in the left or right image within distance limits dependening 
on camera details.

When viewing a 3D point through 2 cameras, if the cameras are not
pointing towards one another, a point in the image of the camera on the right
will be lower horizontally than the image of the point in the left camera.
This disparity is also inversely proportional to the depth of the point.

stereo vision of the rectified images:

                                   #P(x,y,z)
                                 . | .
                               .   |   .
                             .     |     .
                           .       |       .
               <-x_L->   .         |         .  <-x_R->
  image plane |-------# P_L        |       P_R #-------| image plane
                   .               |              .    .
                .                  |                 . f=focal length
              @----------------------------------------@
          opt center          b=Baseline           opt center
            left                                     right
            camera                                   camera

    For each camera, left and right, the focal length f is the perpendicular
    distance from the optical center to the image plane,

    b is the distance between the right and left cameras.

    P is a point in the (x, y, z) reference frame seen as P_L and P_R in the left
    and right images, respectively.
    x_L and x_R are the x coordinates of P_L and P_R.
    Using geometry (similar triangles and parallax), and camera details, one can 
    derive the depth of the point P and know that value relative to the
    camera.  d is that distance for P and is called the disparity for that point.
    
    If the cameras are pinhole cameras, can use similar triangles to
    write
              b * f       b * f
       z = ----------- = -------
            x_L - x_R       d

    Note that the camera should be calibrated so that the objects of interest are
    within a range of distance from the focal plane.
    -------------------------
    calibration of the camera
    -------------------------
       From "3D Video: From Capture to Diffusion" by 
           Laurent Lucas; Celine Loscos; Yannick Remion

       For camera calibration, calculate the projection matrix, then split it
       into sub-matrices to determine focal, position, and direction parameters.

       intrinsic parameters, representing a camera's lens:
               | f_x   s   u_x |
           K = |  0   f_y  u_y |
               |  0    0    1  |

           where f_x and f_y are the focal distance of the camera expressed in 
               pixel units
           s is the non-orthogonality of the sensor's photosite grid, so this
               is usually 0 because the photosite are square. 
           u_x, u_y are coordinates in pixels of the principal point
               (where the optical axis intersects the image).  this is usually
               the image center, but may be offset, especially if zoom is used

       extrinsic parameters represent the position and direction of the camera.

       P_3x4 = K_3x3 * [R_3x3 | -R_3x3*c_3x1]

       where [A | B] is the matrix concatenation of A and B
       and R is the rotation matrix for the camera's direction in the scene's 
           reference frame,
       and c is the camera's position of the camera's center of projection in
           the scene's reference frame.  c is the vector [c_x, c_y, c_z, c_w]^T

       P = K * [R | -R*c]
       P = [K*R | -K*R*c] = [p1 p2 p3 p4]
       let M = [p1 p2 p3]
       then [K*R | -K*R*c] = [M | p4]

           once K and R are solved by QR decomposition of M, 
           the position of the camera is obtained as:
           c_x = det([p2 p3 p4])
           c_y = det([p1 p3 p4])
           c_z = det([p1 p2 p4])
           c_w = det([p1 p2 p3])

           and c = [c_x/c_w, c_y/c_w, c_z/c_w)^T

       Calibration requires multiple pairs of images of a test pattern viewed 
       from different angles.  Need the entire pattern visible in each image 
       without occlusion.   A chess or checker board pattern is common as rulers
       projected to 2 dimensions from several angles and covering vanishing points.

       The test pattern should be at approx the distance of the objects of interest
       to photograph after calibration.   The measurements of the the test pattern,
       such as a checkerboard, should be measured very precisely.

       correspondences in the images can be used to solve for the fundamental
       matrix.   (left image points)^T * (fundamental matrix) * (right image points) = 0
 
       then the essential matrix can be derived from the fundamental matrix or 
       essential solved for from beginning?

-------------------------------------
Making of Disparity Maps (continued):
-------------------------------------
    -- The distance in pixels between corresponding points in the rectified images
       is called disparity. The disparity is used for 3-D reconstruction, because
       it is proportional to the distance between the cameras and the 3-D world point.
       That is, objects w/ larger disparity are closer to the camera.

       from http://www.cs.cornell.edu/~hema/rgbd-workshop-2014/papers/Alhwarin.pdf

    Matching homologous pixels between left and right images is called stereoscopic
    matching.  The otherwise similar pixels present in both images may vary due to
    the lighting, or occlusion, or color variations at the edges of objects due to
    a difference in background objects due to a change in perspective, or due to
    a difference in the size of a patch of pixels in one image and the other due to
    being further or closer to the optical center, or ambiguity due to a pixel
    being in a largely featureless region, or ambiguity due to repetitive patterns.


    There are many algorithms for determining sparse or dense disparity maps.
    Here, will summarize the simplest of methods, the fastest, and then the most 
    accurate but slower algorithms.
        Useful resources for reading were:
           http://en.wikipedia.org/wiki/Binocular_disparity
           http://chrisjmccormick.wordpress.com/2014/01/10/stereo-vision-tutorial-part-i/
           http://vision.middlebury.edu/mview/seitz_mview_cvpr06.pdf
           http://vision.middlebury.edu/stereo/taxonomy-IJCV.pdf
           http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.7321&rep=rep1&type=pdf
           http://www.cvlibs.net/publications/Geiger2010ACCV.pdf
           http://cmp.felk.cvut.cz/~cechj/GCS/
           http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf 

    -- The SIMPLEST algorithm for making a disparity map would be to compare patches
       of a block size in the left image to patches in the right image using a brute force
       comparison of each pixel centered patch in the left image and a range search against
       each pixel centered patch in the right image.
          -- One can improve upon that by retaining the pixel centered patch calculations
             from the right image and reusing them for each left patch comparison.
          -- One can improve upon that by limiting the comparisons to only those 
             in the forward scan directions once one establishes the first best 
             matching patches.
          -- One can limit the number of rows needed in a comparison presumably in a few ways
             including possibly during camera system set up
             (in other words, creating a rectilinear stereo rig).
          -- One could improve upon that by storing the previous left pixel patch calculations
             by columns and rows in a manner to update the patch of comparison
             (more memoization)
          -- One could imagine re-using the points of correspondences used to derive the 
             transformations in the first place as anchors of smaller error disparities
             that be used to define boundaries from which one could further match 
             left and right within the bounds.  Note, one of the fastest current 
             algorithms further written about in the next section, uses different 
             points in such a manner to create a Delanauy Triangulation of 
             bounds to search within to populate the remaining disparity map points.

          The patch block comparisons tend to use a greyscale intensity for the pixel (0-255)
          or color space indexing such as CIE x and y. (The CIE xy color space are independent of
          intensity, but most objects have a color dependent reflection (see index of refraction 
          for materials or albedo for more information).  But if the illumination
          is largely grey scattering of light, that is Mie scattering, CIE xy could be very useful
          for segmentation of related pixels.)

          The 3 different patterns used for comparing the blocks of pixels are for the simplest:
              -- Sum of absolute differences:
                 \sum{\sum{ | L(r,c) - R(r,c-d) | }}
              -- Sum of squared differences:
                 \sum{\sum{ (L(r,c) - R(r,c-d))^2 }}
              -- Normalized correlation:
                 \sum{\sum{ L(r,c) dot R(r,c-d) }}
                 --------------------------------------------------------------
                 \sqrt{(\sum{\sum{ L(r,c)^2 }}) dot (\sum{\sum{ R(r,c-d)^2 }})} 
              
              where r and c are row and column of the  pixel within the left image (L)
              and right image (R).
              d is the disparity.

          The disparity for a pixel is then where those comparison functions are minimized
          and that means that the number of scanned pixels, that is the offset from left and
          right, has to be large enough to find the real disparity for a left pixel.

          Instead of using the minimum, one can interpolate between the smallest values 
          to find the peak of those as a parabola if only using a 1-D scan comparison:
            https://chrisjmccormick.wordpress.com/2014/01/10/stereo-vision-tutorial-part-i/
            d_estimated = d_2 - 0.5*(C_3 - C_1)/(C_1 - 2*C_2 + c_3) 
               where _2 is for the closest matching block and _1 and _3 are those to the left
               and right of it, respectively.

          Note also, that some algorithms use segmentation of either disparity or of color in
          the first step to interpret the disparities as surfaces.  Segmentation, combined
          with growing connected similar regions are sometimes used.

          Algorithms similar to the above tend to have the longest runtimes unless steps
          are taken to make the comparisons smaller in number and size.

    -- The FASTEST algorithms for making a disparity map want to be usable in realtime systems.
        The algorithm with the fastest runtime with a high accuracy seems to be:
            Geiger, Roser, and Urtasun 2010, "Efficient Large-Scale Stereo Matching"
            http://www.cvlibs.net/publications/Geiger2010ACCV.pdf
            The authors offer the code under a GPL license, but only for non-commercial systems.
                http://www.cvlibs.net/download.php?file=libelas.zip
            The algorithm uses a Delanay triangulation of well determined disparities called
            "support points" as boundary points to more efficiently search the disparity space.
            Image boundary "support points" are added with values assigned from their neighbors.
            It assumes the left and right images are already aligned vertically so that
            scaning is only needed along a row.
            They discard ambiguous matches if the 2nd best match is within a threshold 0.9 
            (factor?  difference?) of the first.  They discard matches which look spurious
            by being very different from all of their surrounding support points.
            For each pixel, the support points within a 20x20 region are used to calculate
            a mean which is used to limit the disparity range of the pixel search to less
            than 3 * sigma from the mean (where sigma is presumably the standard deviation
            of the mean).   For those pixels, feature vectors are created using Sobel operators
            in blocks of size 5x5.  the feature vectors are used to generate probabilities of
            matches between the compared left and right image pixels.
            Their generative model: (1) Given support points and left image pixels, draw
            a disparity from the prior probability function which is eqn 2 in the paper.  
            (2) Given the generated disparity and left pixel, draw the right pixel from the 
            constrained Laplacian distribution which uses the feature vectors.  This is eqn
            4 in the paper.  The "draws" are repeated 100 times for each pixel.
            For each left pixel set of generative disparities and right pixels, the estimated
            final disparity for that pixel is found by minimizing an eqn that combines use of
            the log of the prior and constrained laplacian probabilities.
            (Note: the 100 throws might be purely for the paper figure and the impl in code
            might be different?) 
            Note also that the minimization for each pixel's matching r sets of data can be
            completed faster on processors that can process SIMD instructions provided by
            the authors.

            Are there some images in which the calculated "support points" are not 
            a small enough mesh to handle image characteristics such as shadows?
            just noticing the disparity maps for this and SGM and a proposed local plane 
            sweep algorithm in other online slides...

        -- Note that in their paper, the next best performing for the runtime is from the paper
               "Growing Correspondence Seeds.  A Fast Stereo Matching of Large Images" by
               Jan Cech, 2007.
               Their code is offered w/o commercial restrictions, but is in matlab.
               http://cmp.felk.cvut.cz/~cechj/GCS/

    -- Among the most ACCURATE algorithms considering a compromise w/ runtime are the 
            Graph-Cut algorithms.
        One of the latest implementations:
            http://www.csd.uwo.ca/~yuri/Papers/pami04.pdf
            "An Experimental Comparison of Min-Cut/Max-Flow Algorithms for
            Energy Minimization in Computer Vision"
            by Boykov and Kolmogorov 2004
            http://pub.ist.ac.at/~vnk/software/maxflow-v3.03.src.zip
        and before that was V. Kolmogorov and R. Zabih, 2001
            "Computing visual correspondence with occlusions using graph cuts."
            http://pub.ist.ac.at/~vnk/software.html, match-v3.4.src.tar.gz

        Compare where possible to Semi-Global Matching.
           There is a CUDA only Semi-Global Matching algorithm: 
               "Real-time Stereo Vision: Optimizing Semi-Global Matching" by Michael et al. 2013.
               http://www.ini.rub.de/data/documents/michaeletal_improvesgm_iv2013.pdf
           There is Hirschmuller 2005
               but there are mentions of using it in textureless patches or where dense 
               disparity maps are needed.

    In navigation systems with very high resolution images, wouldn't an early 
         down sampled image pair solution much faster than 1 second be useful 
         before a more accurate dense disparty map that might take 10 seconds?
         (see the Disney mansion runtimes in 
         http://www.imgfsr.com/CVPR2014IRW/I1.pdf
         http://research.microsoft.com/pubs/232423/sinhaLPS_CVPR2014.pdf)
         There would be large errors per down sized image pixel that could be
         used to estimate a possible range of true distances around the quick 
         determination for an early look around.

-----------------
3-D point clouds:
-----------------
    The reprojection from the disparity map to a 3D point cloud is
        Q * [x y d 1]^T = [X Y Z W]^T
        that is the 2D point is reprojected to 3D space as [X/W Y/W Z/W]^T

    in progress...

