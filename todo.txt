Note, this is a precursor step to the agile story and tasks to gather notes 
on needed features, bugs, etc.

-- consider making a separate method in EdgeExtractor for junction finding
   between all edge points, even if embedded in the curve.
   -- O(N_points) calculate x and y bounds of each edge
   -- < O(N_points^2) compare edges within 1 pixel of each other's bounds to find
      adjacent points.
      store in 2 maps: key = pair int of pix coord, value = set<PairInt> of adjacent.
                       key = pair int of each junction pix coord, 
                             value = pairint holding edge index and index within the edge
   -- << O(N_points) iterate over the junction maps to remove the keys which are not the center
      of their junctions by comparing the number of values in first map among all members.

-- since the Edges are constructed from intensity differences as the basis of the gradient,
   it seems as if it might be useful to make a similar gradient set of images based upon
   color instead of intensity.  
   to do so, one could use the CIE X,Y color space with a center of (0.35, 0.35) and calculate
   the theta of a (cieX, xieY) point.  that theta between 0 and 359 degrees would be mapped
   to values 0 to 254.  the center space near (0.35, 0.35) is white so would be set to value 255.
   I started the code for this already.
   -- converting color image to cie xy theta creates a greyscale image which can be used
      by the CannyEdgeFilter and EdgeExtractor similarly to the intensity greyscale image.
   -- compare the resulting extracted edges between the default intensity based and a color based.


-- after all of the improvements to the EdgeExtractor are finished,
   revisit the CannyEdgeFilter to adjust the 2-layer threshold,
   and consider adjusting the gaussian convolution radius.
   -- then consider adjustments in the Corner detector's curvature
      factor above minimum.
   -- rerun curvature and inflecion tests

-- finish integrating use of skyline extractor in edge extractor
   
-- return to the point correspondence

-- for coverage reports, re-enable
       SkylineDownhillSimplexTest
    
-- before merging with main, checkout code and build and runtests
   in empty directory to make sure that all files have been checked in.

-- for a datastructure ordered by x and y:
Notes: A "celltree" is now more commonly called a "compressed quadtree" or "compressed hyperoctree", and this paper is apparently the first appearance of such a construction. Vaidya refined the algorithm here to avoid randomness and bit-twiddling, at some cost in dependence on d; his algorithm, and this one, and that of Gabow, Bentley, and Tarjan, all use the same basic geometrical observation, which implies that the total number of nearest neighbors is O(n), even up to approximate neighbors. Callahan and Kosaraju used similar ideas for "well-separated pairs decomposition". The σ ratio is now more commonly called the spread.

-- when sun is found, such as in the NM image, might try to fit a
   radial profile (1/r^2) from it over the image.
   can wait until sky pixels are mostly found and then fit over
   that and extend that pattern to the rest of the image?
   that is, in the NM image, can see that the sky varies smoothly
   but there are clouds that are in the sky too. 
   ** what happens when one knows the sun is in the image,
   one fits a gradient to the least bright pixels and then divides
   the image by that gradient?  presumably, the skyline stands out
   even more.   cie XY are independent of illumination in any case,
   so this step probably isn't useful and not always possible...

-- include other atmospheric optically visible features:  sun dogs and moon dogs.
-- consider more images which are near evening or evening, but there is enough illumination
   to see a skyline.

-- consider simplest ways to find solar reflected light in the images:
     -- already have a method which finds visible sun's photosphere
     -- create a separate method for reflected from water (see earlier code... depending upon sun location,
        the light might be orange to white...)... can use a source function of the sun
        and mie and rayliegh scattering if more formal methods are needed and reflection off of optically
        thick H_2O.
     -- refracted and reflected from clouds that are thick enough to provide a surface of reflected light
        that has a pattern of brightness due to illumination by sun...
     -- consider whether the location of the sun when photosphere is not in image is learnable
        by gradient of brightness.

-- add more tests when have a working solution for skyline extraction
   -- test by resolution too.  the seed sky points probably have some resolution dependence at this point.
   -- find test images for:
         -- structured cloudy sky and smooth white foreground
         -- structured cloudy sky and smooth dark foreground
         Neither of those will be solved as well as the NM test image which had non-sky foreground colors.
         Need additional information to understand what is sky without external sensors
         or assumption of horizontal.  In ambiguous cases, might need to make the assumption of
         skyline being horizontal in the image and sky near the top of the image...would like to avoid
         that assumption if at all possible.

--in the RANSAC stage, for a threshold for matching, consider using
   0.6% of maximum image dimension
   (http://phototour.cs.washington.edu/ModelingTheWorld_ijcv07.pdf)

-- revisit my RANSAC methods and consider characteristics from various
   descriptions of implementations.  recalculating the number of iterations
   as the number of inliers and outliers are determined.

-- there's a 5-point method:
   Nister, D.: An efficient solution to the five-point relative pose problem. Trans.
   PAMI 26, 756–770 (2004)

-- one day, put in my algorithms toolbox, deconvolution methods:
   weiner filter deconvolution
   Tikhonov regularization
   Richardson-Lucy

-- find image with naturally occuring skew for tests

-- for ways to reduce the corner list, consider reading:
   Shi, J., and C. Tomasi. "Good Features to Track." 
   Conference on Computer Vision and Pattern Recognition, 1994.

   journals to browse:

   CVGIP  Graphical Models /graphical Models and Image Processing /computer Vision, Graphics, and Image Processing
   CVIU  Computer Vision and Image Understanding
   IJCV  International Journal of Computer Vision
   IVC  Image and Vision Computing
   JMIV  Journal of Mathematical Imaging and Vision
   MVA  Machine Vision and Applications
   TMI - IEEE  Transactions on Medical Imaging

   http://pointclouds.org/documentation/

-- read:
    http://proquest.safaribooksonline.com.ezproxy.spl.org:2048/book/illustration-and-graphics/9780133373721

-- revisit the matrix math, especially where there's multiplication.
   did I replace dot operation with multiplication anywhere?
   needs tests...

-- reconsider later some properties.
   hue for blue skies.
   eucl dist of color = sqrt((r-r)^2 + (g-g)^2 + (b-b)^2)
   
another test image?
   http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300/html/dataset/images/color/260058.html

-- read more on segmentation... lots of comparisons on the benchmark site.
   note the skyline extractor here is only partial segmentation and specific to a task.

   http://www.cs.berkeley.edu/~arbelaez/publications/amfm_pami2011.pdf

-- test datasets
http://sipi.usc.edu/database/database.php?volume=misc
http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#segmentation

-- make a line filter test for erosion filter
   which extends from left to right of image.

-- suddenly, the brown & lowe are not well matched, so return to this
   after the higher priority tasks.

-- consider changing canny edge iteratively to iteratively if over critical limit
   ... I think this is comment made when I added ability to increase the convolution
       radius in order to reduce the number of corners when requested.

-- when everything is working correctly in point matcher,
   consider speeding it up with a quick extremely rough grid
   search to narrow further search space.
   rot=0, 90, 180, 270 and scale=1, 2, and 4

-- finish the method converting from fundamental matrix to essential matrix.

-- consider following the implementation of disparity maps for stereo images
   and 3d modelling.  see notes in the docs directory.
   -- see http://vision.middlebury.edu/stereo/code/

-- consider implementing the LM-ICP from fitz-gibbons?
   did i look at this already and decide otherwise?
   does is handle projective transformations?

-- ** read: https://www.graphics.rwth-aachen.de/person/21/

-- for tests, make sure the projective point matcher can handle images like the
    ones in this tutorial:
    http://www.robots.ox.ac.uk/~az/tutorials/tutorialb.pdf

-- to the corner list makin',
   -- add a removal of redundant points

-- test for degenerate camera conditions:
   -- parallel camera motion w/o rotation 
-- test for degenerate scene structure configurations
   -- all points lying on a plane or nearly lying on a plane (?)
-- test for point sets containing noise
-- error estimate in fundamental matrix:
   6.5.3
   -- Gold Standard algorithm?  (need camera details...)
      summation over i( d(x_i, x_hat_i)^2 + d(x_prime_i, x_prime_hat_i)^2 ) 
   -- Sampson distance?

http://dev.ipol.im/~morel/Dossier_MVA_2011_Cours_Transparents_Documents/2011_Cours7_Document3_hartley.pdf
-- a hartley paper suggests rectifying images to evaluate point transformations.

