Note, this is a precursor step to the agile story and tasks to gather notes 
on needed features, bugs, etc.

-- organizing the SkylineExtractor's findClouds parameters for consideration of optimization.

   For algorithm filter parameters note that diff is between pixel and it's 8 neighbor sky-only-pixels.

   All of the clauses should evaluate to 'T' is a sky pixel else 'F' is not a sky pixel.

       ----------------------------------------------------------------------------------------------------------------------
       parameter                     should be present      first look at clauses to fit
       ----------------------------------------------------------------------------------------------------------------------
       diffRed                             Y                (param < coeff), ((param/stdev) < coeff)
                                                                ((Math.abs(param) < coeff), ((Math.abs(param)/stdev) < coeff)
       diffBlue                            Y                (param < coeff), ((param/stdev) < coeff)
                                                                ((Math.abs(param) < coeff), ((Math.abs(param)/stdev) < coeff)
                                                            NOTE that some pre-logic could be used to use only diffBlue or
                                                            diffRed depending on sky color to make a parameter diffBorR
       skyIsBlue                           Y                (param == true), (param == false), param not used in clause
       skyIsRed                            N                this is the same as the opposite of skyIsBlue
       skyIsBrown                          Y                (param == true), (param == false)
       contrast                            Y                (param < coeff), ((param/stdev) < coeff), 
                                                                ((Math.abs(param) < coeff), ((Math.abs(param)/stdev) < coeff)
       diffCIEX                            Y                (param < coeff), ((param/stdev) < coeff)
                                                                ((Math.abs(param) < coeff), ((Math.abs(param)/stdev) < coeff)
                                                                because cie is a radial polynomial, might actually need
                                                                something like polar coords. **theta** should be the 
                                                                most important measure of the difference
                                                                diffCIEXY = sqrt(diffCIEX*diffCIEX + diffCIEY*diffCIEY)
                                                                theta = 2*arctan(diffCIEY/(diffCIEX + diffCIEXY))

       diffCIEY                            Y                (param < coeff), ((param/stdev) < coeff)
                                                                ((Math.abs(param) < coeff), ((Math.abs(param)/stdev) < coeff)
                                                                
       colorDiff                           Y                (param < coeff), ((param/stdev) < coeff)
                                                                ((Math.abs(param) < coeff), ((Math.abs(param)/stdev) < coeff)
                                                                note, this is defined using r, g, and b
       skyStDevContrast                    Y                see above
       skyStDevColorDiff                   Y                see above
       skyStDevCIEX                        Y                see above
       skyStDevCIEY                        Y                see above
       isSolarYellowGreenOrange            Y                (param == true), (param == false)
       rDivTotal                           Y                Math.abs(coeff - rDivTotal) < coeff2
       gDivTotal                           Y                Math.abs(coeff - gDivTotal) < coeff2
       bDivTotal                           Y                Math.abs(coeff - bDivTotal) < coeff2

       If each pameter can be in one of m states, the number of possible combinations for one
       parameter is m_states^(1 parameter).  And combining parameters is multiplication.
       For example, if we had 3 parameters and we knew each could be in one of 2 states, the
       number of possible combinations is 2^3.  Combining them separately is (2^1)*(2^1)*(2^1).
       That means that for 3 parameters with 2 states each, one would be running 8 separate
       total equations of clauses to optimize separately (and afterwards decide between the results of).

       Note that the logic of the problem itself is specialization of segmentation that is very specific
       to sky, land, and foreground.  For that reason, some of the logic is best applied in an order.
       More specifically, a filter for brown land is performed before the other
       operations to quickly exclude land.
       Also, filter for large contrast should be applied early on to rule out 'sky' as a pixel.
       And there is a trend for blue sky and red sky that should be applied, that is for blue
       sky, the difference in contrast and difference in blue make a jump at skyline boundaries,
       while for red skies, the skyline is remarkable in the difference in contrast and the
       difference in red color.
           Note, for this color diff and contrast diff relationship, no simple coefficient fit
           a trend usable for all of them, that is a simple linear relationship was not correct.
           one can see from the plots that the contrast scales with the difference in color, 
           which must be because contrast is built from luma, so roughly an intensity difference 
           and the difference in color is a difference in intensity.
           *Note that I didn't plot a difference in CIE XY chromaticity in the same way, but one 
           would expect cases where that might fail? especially for snowy mountain peaks under 
           hazy skies... CIE XY is color without luminosity dependency.
           *Note also that strength of the contrast is different for every object because it's
           dependent upon it's light scattering properties such as index of refraction for its
           materials.
           so the comparison is probably just a minimum factor above the standard deviation 
           of that property for the sky.

    Once all of the equations of clauses are calculated, can solve for the parameters 
    using many test images.
    Because there are so many parameters, a gradient approach is less appealing than a heuristic
    approach such as the Nelder-Mead downhill simplex method.   
        set the initial coefficients with a good approximation such as is currently in the code 
        so that this is a local search.
        for each equation (where an equation is the set of clauses of parameters and coefficients):
            iterate until convergence or maxIter:
                for each test image: 
                    use findClouds w/ given coefficients.
                    the result is a set of sky points.
                    calculate the residual difference between result and the expected sky set.
                    store the best result for the image for the equation. 
                improve the coefficients based upon each image's best solution.
                this is done in the Nelder-Mead algorithm.
                evaluate whether has converged
        having stored the best result for each image for each equation, combine and compare
           the best results for each equation over all images and choose the
           best or combine the solutions to make the best solution for each equation.
        then the solution is the best among the equations' solutions.

        note that the fitness function should probably be weighted by the expected number of
        sky points so that it's actually measuring the fraction found as sky

-- improvement in design of SkylineExtractor:
   consider retaining the pixels which do not get added to sky in
   findClouds as border pixels.
     -- do a quick plot to assert that they are border and embedded pixels.
     -- for the subsequent stage of finding embedded pixels, shoud be able
        to reduce the number of operations, by finding the contiguous
        pixels only among those border pixels, but still using PerimeterFinder's methods 
        on these alternately derived border pixels.
        follow up: still need the approx concave hull for the check
        whether the contiguous pixels are bounded... **might only be saving
        steps in the small block where gaps are iterated over to write embedded pixels.
        -- the result is the embedded pixels.
        -- the true border pixels will take the border pixels just mentioned
           and subtract the embedded subset from them.


-- when sun is found, such as in the NM image, might try to fit a
   radial profile (1/r^2) from it over the image.
   can wait until sky pixels are mostly found and then fit over
   that and extend that pattern to the rest of the image?
   that is, in the NM image, can see that the sky varies smoothly
   but there are clouds that are in the sky too. 
   ** what happens when one knows the sun is in the image,
   one fits a gradient to the least bright pixels and then divides
   the image by that gradient?  presumably, the skyline stands out
   even more.   cie XY are independent of illumination in any case,
   so this step probably isn't useful and not always possible...

-- the rainbow points can be found before findClouds because it is a search over
   the entire image.

   -- keep the current steps to store the ellipse parameters and rainbow points 
      and make the plot

   then consider:
      -- encapsulation of the rainbow points can be accomplished with a 
         convex hull.  then to include all rainbow pixels, would want to find the
         maximum width or the mode of the width (where width is the dimension of the
         rainbow perperdicular to the half ellipse) and expand the hull to that width
         all along the main axis.
      -- interpolation over that hull region with left and right samples surrounding it
         making sure that the same amount of color and brightness variation is present
         so that later use of the method findClouds interprets the pixels as sky.

   then the rest of the sky extraction can proceed normally.
      should not need to correct for rainbow intersection at skyline, but if so,
      can use method similar to used in sun corrections, that is, follow the skyline
      match in the theta image, and in this case, interpolate between left and right
      of rainbow.

-- include other atmospheric optically visible features:  sun dogs and moon dogs.
-- consider more images which are near evening or evening, but there is enough illumination
   to see a skyline.

-- consider revisiting solveForBestFittingContiguousSubSamples in PolynomialFitter to
   make more tests.  walk through the method and check the math.

-- if there are still more stragglers of clouds connected to the boundaries of the image, 
   consider something
   like the spur removal, but only for pixels the same color as the sky...
   have to make sure only use such an algorithm when the image boundary pixels are sky on either side
   to avoid growing the foreground.

-- consider simplest ways to find solar reflected light in the images:
     -- already have a method which finds visible sun's photosphere
     -- create a separate method for reflected from water (see earlier code... depending upon sun location,
        the light might be orange to white...)... can use a source function of the sun
        and mie and rayliegh scattering if more formal methods are needed and reflection off of optically
        thick H_2O.
     -- refracted and reflected from clouds that are thick enough to provide a surface of reflected light
        that has a pattern of brightness due to illumination by sun...
     -- consider whether the location of the sun when photosphere is not in image is learnable
        by gradient of brightness.

-- add more tests when have a working solution for skyline extraction
   -- test by resolution too.  the seed sky points probably have some resolution dependence at this point.
   -- find test images for:
         -- structured cloudy sky and smooth white foreground
         -- structured cloudy sky and smooth dark foreground
         Neither of those will be solved as well as the NM test image which had non-sky foreground colors.
         Need additional information to understand what is sky without external sensors
         or assumption of horizontal.  In ambiguous cases, might need to make the assumption of
         skyline being horizontal in the image and sky near the top of the image...would like to avoid
         that assumption if at all possible.

-- junction corners should be handled in edge extractor.  for large number of edges, might have
   more fractured lines, hence missing corners at junctions.

--in the RANSAC stage, for a threshold for matching, consider using
   0.6% of maximum image dimension
   (http://phototour.cs.washington.edu/ModelingTheWorld_ijcv07.pdf)

-- revisit my RANSAC methods and consider characteristics from various
   descriptions of implementations.  recalculating the number of iterations
   as the number of inliers and outliers are determined.

-- there's a 5-point method:
   Nister, D.: An efficient solution to the five-point relative pose problem. Trans.
   PAMI 26, 756–770 (2004)

-- one day, put in my algorithms toolbox, deconvolution methods:
   weiner filter deconvolution
   Tikhonov regularization
   Richardson-Lucy

-- find image with naturally occuring skew for tests

-- for ways to reduce the corner list, consider reading:
   Shi, J., and C. Tomasi. "Good Features to Track." 
   Conference on Computer Vision and Pattern Recognition, 1994.

   journals to browse:

   CVGIP  Graphical Models /graphical Models and Image Processing /computer Vision, Graphics, and Image Processing
   CVIU  Computer Vision and Image Understanding
   IJCV  International Journal of Computer Vision
   IVC  Image and Vision Computing
   JMIV  Journal of Mathematical Imaging and Vision
   MVA  Machine Vision and Applications
   TMI - IEEE  Transactions on Medical Imaging

   http://pointclouds.org/documentation/

-- read:
    http://proquest.safaribooksonline.com.ezproxy.spl.org:2048/book/illustration-and-graphics/9780133373721

-- revisit the matrix math, especially where there's multiplication.
   did I replace dot operation with multiplication anywhere?
   needs tests...

-- reconsider later some properties.
   hue for blue skies.
   eucl dist of color = sqrt((r-r)^2 + (g-g)^2 + (b-b)^2)
   
another test image?
   http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300/html/dataset/images/color/260058.html

-- read more on segmentation... lots of comparisons on the benchmark site.
   note the skyline extractor here is only partial segmentation and specific to a task.

   http://www.cs.berkeley.edu/~arbelaez/publications/amfm_pami2011.pdf

-- test datasets
http://sipi.usc.edu/database/database.php?volume=misc
http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm#segmentation

-- make a line filter test for erosion filter
   which extends from left to right of image.

-- suddenly, the brown & lowe are not well matched, so return to this
   after the higher priority tasks.

-- consider changing canny edge iteratively to iteratively if over critical limit
   ... I think this is comment made when I added ability to increase the convolution
       radius in order to reduce the number of corners when requested.

-- when everything is working correctly in point matcher,
   consider speeding it up with a quick extremely rough grid
   search to narrow further search space.
   rot=0, 90, 180, 270 and scale=1, 2, and 4

-- finish the method converting from fundamental matrix to essential matrix.

-- consider following the implementation of disparity maps for stereo images
   and 3d modelling.  see notes in the docs directory.
   -- see http://vision.middlebury.edu/stereo/code/

-- consider implementing the LM-ICP from fitz-gibbons?
   did i look at this already and decide otherwise?
   does is handle projective transformations?

-- ** read: https://www.graphics.rwth-aachen.de/person/21/

-- for tests, make sure the projective point matcher can handle images like the
    ones in this tutorial:
    http://www.robots.ox.ac.uk/~az/tutorials/tutorialb.pdf

-- to the corner list makin',
   -- add a removal of redundant points

-- test for degenerate camera conditions:
   -- parallel camera motion w/o rotation 
-- test for degenerate scene structure configurations
   -- all points lying on a plane or nearly lying on a plane (?)
-- test for point sets containing noise
-- error estimate in fundamental matrix:
   6.5.3
   -- Gold Standard algorithm?  (need camera details...)
      summation over i( d(x_i, x_hat_i)^2 + d(x_prime_i, x_prime_hat_i)^2 ) 
   -- Sampson distance?

http://dev.ipol.im/~morel/Dossier_MVA_2011_Cours_Transparents_Documents/2011_Cours7_Document3_hartley.pdf
-- a hartley paper suggests rectifying images to evaluate point transformations.

